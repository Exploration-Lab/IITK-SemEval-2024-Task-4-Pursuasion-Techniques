{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fc4f36-1be8-4e70-aba9-fbb0ea186624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "## Training set preperation\n",
    "dict_mapping = {'Presenting Irrelevant Data (Red Herring)': 0,\n",
    " 'Bandwagon': 1,\n",
    " 'Smears': 2,\n",
    " 'Glittering generalities (Virtue)': 3,\n",
    " 'Causal Oversimplification': 4,\n",
    " 'Whataboutism': 5,\n",
    " 'Loaded Language': 6,\n",
    " 'Exaggeration/Minimisation': 7,\n",
    " 'Repetition': 8,\n",
    " 'Thought-terminating cliché': 9,\n",
    " 'Name calling/Labeling': 10,\n",
    " 'Appeal to authority': 11,\n",
    " 'Black-and-white Fallacy/Dictatorship': 12,\n",
    " 'Obfuscation, Intentional vagueness, Confusion': 13,\n",
    " 'Reductio ad hitlerum': 14,\n",
    " 'Appeal to fear/prejudice': 15,\n",
    " \"Misrepresentation of Someone's Position (Straw Man)\": 16,\n",
    " 'Flag-waving': 17,\n",
    " 'Slogans': 18,\n",
    " 'Doubt': 19,\n",
    " 'Bandwagon_1': 20,\n",
    " 'Whataboutism_1': 21,\n",
    " 'Appeal to fear/prejudice_1': 22,\n",
    " 'Flag-waving_1': 23,\n",
    " 'Appeal to authority_1': 24,\n",
    " \"Transfer_1\" : 25,\n",
    " \"Transfer\" : 26,\n",
    " \"Appeal to (Strong) Emotions\" : 27\n",
    "}\n",
    "\n",
    "\n",
    "def dataset_f(data,dict_mapping):\n",
    "    set_x = set()\n",
    "    for element in data:\n",
    "        for label in element['labels']:\n",
    "            set_x.add(label)\n",
    "    \n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"labels\"] = element[\"labels\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        dict_x[\"image\"] = element[\"image\"]\n",
    "        if len(element['labels']):\n",
    "            data_id.append(dict_x)\n",
    "    \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    image_ID = []\n",
    "    for element in data_id:\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        pattern = r'\\\\n'\n",
    "        cleaned_text = re.sub(pattern, ' ',cleaned_text)\n",
    "        pattern = r'[^a-zA-Z0-9\\s]+'\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text).lower()\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        if(cleaned_text != ''):\n",
    "            unprocessed.append(element['text'])\n",
    "            processed.append(cleaned_text)\n",
    "            labels.append(element['labels'])\n",
    "            ID.append(element['id'])\n",
    "            image_ID.append(element['image'])\n",
    "            \n",
    "        \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID,'image_ID' : image_ID}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        l_labels = row['label']\n",
    "        empty_list = []\n",
    "        for label in l_labels:\n",
    "            empty_list.append(dict_mapping[label])\n",
    "        row['label'] = empty_list\n",
    "\n",
    "    return df\n",
    "    \n",
    "with open(r'train.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_train = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'validation.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_val = dataset_f(data,dict_mapping)\n",
    "\n",
    "\n",
    "df_train_full = pd.concat([df_train,df_val],axis=0)\n",
    "df_exploded_train = df_train_full.explode('label')\n",
    "\n",
    "condition = df_exploded_train['label'].isin([1, 5, 15, 17, 11, 26])\n",
    "df_duplicated = df_exploded_train[condition].copy()\n",
    "mapping_dict = {1: 20, 5: 21, 15: 22, 17: 23, 11: 24,26 : 25}\n",
    "df_duplicated['label'] = df_duplicated['label'].map(mapping_dict)\n",
    "df_duplicated = df_duplicated.drop_duplicates()\n",
    "result_df = pd.concat([df_exploded_train, df_duplicated], ignore_index=True)\n",
    "result_df.to_csv('Merged_train_for_HypEmo_2b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb126af-a965-42b8-a5ef-f49d79ade842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dataset_f(data,dict_mapping):\n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        dict_x[\"image\"] = element[\"image\"]\n",
    "        data_id.append(dict_x)\n",
    "        \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    image_ID = []\n",
    "    \n",
    "    print(len(data_id))\n",
    "    for element in data_id:\n",
    "        unprocessed.append(element['text'])\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        processed.append(cleaned_text)\n",
    "        x = 9\n",
    "        labels.append(x)\n",
    "        ID.append(element['id'])\n",
    "        image_ID.append(element['image'])\n",
    "    \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID,'image_ID' : image_ID}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# with open(r'data_given_to_work_with/subtask1/dev_subtask1_en.json', 'rb') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "# df_dev_eng_full = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'test_data/en_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_eng_full = dataset_f(data,dict_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2285bf0-3860-41b5-b252-731d8f68051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:25<00:00,  3.22s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 1249.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('test_data/ar_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_arab =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_arab[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True))    \n",
    "df_test_arab[\"aug_text\"] = t_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32fe558-fe08-49bd-8cb0-1ff09671cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>image_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مشايخ الأوقاف واقفين لحماية المسجد\\nحماية المس...</td>\n",
       "      <td>The Waqf Chiefs are standing to protect the mo...</td>\n",
       "      <td>9</td>\n",
       "      <td>00001</td>\n",
       "      <td>prop_meme_00001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>المسؤولين العرب بعدما يخرو على شعبهم و حان الآ...</td>\n",
       "      <td>Arab officials, after they've fallen on their ...</td>\n",
       "      <td>9</td>\n",
       "      <td>00002</td>\n",
       "      <td>prop_meme_00002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>عندما يسأل بوتفليقة عن المدة التي يريد قضاءها ...</td>\n",
       "      <td>When Pottflika asks how long Renessa wants to ...</td>\n",
       "      <td>9</td>\n",
       "      <td>00003</td>\n",
       "      <td>prop_meme_00003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لا يعني انه اذا حطيت ايدي عليكي او اتطلعت ع صد...</td>\n",
       "      <td>Doesn't mean that if I put Eddie on you or loo...</td>\n",
       "      <td>9</td>\n",
       "      <td>00004</td>\n",
       "      <td>prop_meme_00004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*اسرائيل*\\n*قتل وهدم واعتقالات\\n* مستوطنات\\n*ض...</td>\n",
       "      <td>Israel's killing, demolition and arrests of se...</td>\n",
       "      <td>9</td>\n",
       "      <td>00005</td>\n",
       "      <td>prop_meme_00005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>هذا القط حياته بائسه جدا وفقيرلايملك ربع تونه💔\\n</td>\n",
       "      <td>This cat's life is so miserable and poor, he d...</td>\n",
       "      <td>9</td>\n",
       "      <td>00499</td>\n",
       "      <td>prop_meme_00499.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>| هى كورونا دى داجل ولا ست\\n والله يابنتى طالم...</td>\n",
       "      <td>It's Corona de Doughle and La Six, and God, my...</td>\n",
       "      <td>9</td>\n",
       "      <td>00506</td>\n",
       "      <td>prop_meme_00506.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>بتضرب أعز أصحابك مقابل مليون دولار؟؟\\nأنا:</td>\n",
       "      <td>By hitting your best friend for a million doll...</td>\n",
       "      <td>9</td>\n",
       "      <td>00541</td>\n",
       "      <td>prop_meme_00541.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ولادي بعد ما تنمرت على نص الكوكب</td>\n",
       "      <td>My birth after you bullied on the planet's text.</td>\n",
       "      <td>9</td>\n",
       "      <td>00542</td>\n",
       "      <td>prop_meme_00542.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>لما تتأخر بالرجعة عالبيت وابوك يقلك وينك لهلأ ...</td>\n",
       "      <td>When you're back home late, your dad picks you...</td>\n",
       "      <td>9</td>\n",
       "      <td>00573</td>\n",
       "      <td>prop_meme_00573.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    مشايخ الأوقاف واقفين لحماية المسجد\\nحماية المس...   \n",
       "1    المسؤولين العرب بعدما يخرو على شعبهم و حان الآ...   \n",
       "2    عندما يسأل بوتفليقة عن المدة التي يريد قضاءها ...   \n",
       "3    لا يعني انه اذا حطيت ايدي عليكي او اتطلعت ع صد...   \n",
       "4    *اسرائيل*\\n*قتل وهدم واعتقالات\\n* مستوطنات\\n*ض...   \n",
       "..                                                 ...   \n",
       "115   هذا القط حياته بائسه جدا وفقيرلايملك ربع تونه💔\\n   \n",
       "116  | هى كورونا دى داجل ولا ست\\n والله يابنتى طالم...   \n",
       "117         بتضرب أعز أصحابك مقابل مليون دولار؟؟\\nأنا:   \n",
       "118                   ولادي بعد ما تنمرت على نص الكوكب   \n",
       "119  لما تتأخر بالرجعة عالبيت وابوك يقلك وينك لهلأ ...   \n",
       "\n",
       "                                              aug_text  label     id  \\\n",
       "0    The Waqf Chiefs are standing to protect the mo...      9  00001   \n",
       "1    Arab officials, after they've fallen on their ...      9  00002   \n",
       "2    When Pottflika asks how long Renessa wants to ...      9  00003   \n",
       "3    Doesn't mean that if I put Eddie on you or loo...      9  00004   \n",
       "4    Israel's killing, demolition and arrests of se...      9  00005   \n",
       "..                                                 ...    ...    ...   \n",
       "115  This cat's life is so miserable and poor, he d...      9  00499   \n",
       "116  It's Corona de Doughle and La Six, and God, my...      9  00506   \n",
       "117  By hitting your best friend for a million doll...      9  00541   \n",
       "118   My birth after you bullied on the planet's text.      9  00542   \n",
       "119  When you're back home late, your dad picks you...      9  00573   \n",
       "\n",
       "                image_ID  \n",
       "0    prop_meme_00001.jpg  \n",
       "1    prop_meme_00002.jpg  \n",
       "2    prop_meme_00003.jpg  \n",
       "3    prop_meme_00004.jpg  \n",
       "4    prop_meme_00005.jpg  \n",
       "..                   ...  \n",
       "115  prop_meme_00499.jpg  \n",
       "116  prop_meme_00506.jpg  \n",
       "117  prop_meme_00541.jpg  \n",
       "118  prop_meme_00542.jpg  \n",
       "119  prop_meme_00573.jpg  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_arab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc16571-c447-4cd0-ad7e-bfe00f59104b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/55 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 55/55 [04:22<00:00,  4.77s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 436/436 [00:00<00:00, 1113.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('test_data/bg_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_bulg =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_bulg[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-bg-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 8\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())    \n",
    "df_test_bulg[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f609244c-f0ce-4b10-9ec1-e7ec453d6ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>image_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ДНЕВНИК\\n\\nКАПИТАЛ\\n\\nAMERICA FOR BULGARIA FOU...</td>\n",
       "      <td>derivative tier 1 america for bulgaria foundat...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_83</td>\n",
       "      <td>bg_mk_memes_83.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Когато съм се ваксинирал и се върна на село да...</td>\n",
       "      <td>when i was vaccinated and returned to the vill...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_104</td>\n",
       "      <td>bg_mk_memes_104.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ганя, ваксинирай се, глупако!!\\n\\nГаня, полудя...</td>\n",
       "      <td>ganya, you're inoculating yourself, you idiot!</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_106</td>\n",
       "      <td>bg_mk_memes_106.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Не можеш ли да работиш нещо нормално като друг...</td>\n",
       "      <td>can't you work something normal like other peo...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_148</td>\n",
       "      <td>bg_mk_memes_148.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Не назеления сертификат\\n\\nВаксиниран съм\\nПри...</td>\n",
       "      <td>no green certificate i've been vaccinated i've...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_237</td>\n",
       "      <td>bg_mk_memes_237.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>@MEME PARTIQ\\nМакедонска\\nСветовна\\nистория\\nи...</td>\n",
       "      <td>@mememe partiq macedonian world history</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>bg_memes_74.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Никога не е била!\\nЧакай, Земята не\\nе македон...</td>\n",
       "      <td>wait, the earth isn't macedonian?</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>bg_memes_75.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Тате Тате || даа\\nАз съм\\nМакедонфц\\nthe\\nЩе г...</td>\n",
       "      <td>dad, i'm the macedonfz, you're gonna burn at y...</td>\n",
       "      <td>9</td>\n",
       "      <td>76</td>\n",
       "      <td>bg_memes_76.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>МАКЕДОНИЯ и\\nБЪЛГАРИЯ\\nМАКЕДОНИЯ и\\n- БЪЛГАРИЯ...</td>\n",
       "      <td>macedonia and bulgaria macedonia and - bulgari...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>bg_memes_8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>АМЕРИКАНСКАТА МЕЧТА\\nglasspar\\nDoughboy\\nCHATI...</td>\n",
       "      <td>american bears glasspar doughboy chaties chiem...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_memes_9.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ДНЕВНИК\\n\\nКАПИТАЛ\\n\\nAMERICA FOR BULGARIA FOU...   \n",
       "1    Когато съм се ваксинирал и се върна на село да...   \n",
       "2    Ганя, ваксинирай се, глупако!!\\n\\nГаня, полудя...   \n",
       "3    Не можеш ли да работиш нещо нормално като друг...   \n",
       "4    Не назеления сертификат\\n\\nВаксиниран съм\\nПри...   \n",
       "..                                                 ...   \n",
       "431  @MEME PARTIQ\\nМакедонска\\nСветовна\\nистория\\nи...   \n",
       "432  Никога не е била!\\nЧакай, Земята не\\nе македон...   \n",
       "433  Тате Тате || даа\\nАз съм\\nМакедонфц\\nthe\\nЩе г...   \n",
       "434  МАКЕДОНИЯ и\\nБЪЛГАРИЯ\\nМАКЕДОНИЯ и\\n- БЪЛГАРИЯ...   \n",
       "435  АМЕРИКАНСКАТА МЕЧТА\\nglasspar\\nDoughboy\\nCHATI...   \n",
       "\n",
       "                                              aug_text  label  \\\n",
       "0    derivative tier 1 america for bulgaria foundat...      9   \n",
       "1    when i was vaccinated and returned to the vill...      9   \n",
       "2       ganya, you're inoculating yourself, you idiot!      9   \n",
       "3    can't you work something normal like other peo...      9   \n",
       "4    no green certificate i've been vaccinated i've...      9   \n",
       "..                                                 ...    ...   \n",
       "431            @mememe partiq macedonian world history      9   \n",
       "432                  wait, the earth isn't macedonian?      9   \n",
       "433  dad, i'm the macedonfz, you're gonna burn at y...      9   \n",
       "434  macedonia and bulgaria macedonia and - bulgari...      9   \n",
       "435  american bears glasspar doughboy chaties chiem...      9   \n",
       "\n",
       "                  id             image_ID  \n",
       "0     bg_mk_memes_83   bg_mk_memes_83.png  \n",
       "1    bg_mk_memes_104  bg_mk_memes_104.png  \n",
       "2    bg_mk_memes_106  bg_mk_memes_106.png  \n",
       "3    bg_mk_memes_148  bg_mk_memes_148.png  \n",
       "4    bg_mk_memes_237  bg_mk_memes_237.png  \n",
       "..               ...                  ...  \n",
       "431               74      bg_memes_74.png  \n",
       "432               75      bg_memes_75.png  \n",
       "433               76      bg_memes_76.png  \n",
       "434                8       bg_memes_8.png  \n",
       "435                9       bg_memes_9.png  \n",
       "\n",
       "[436 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_bulg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e0b4d6-c395-4940-a188-4f465fe54019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0849bb03-0cf7-47c0-a7ba-a118da896277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:23<00:00,  1.40s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 259/259 [00:00<00:00, 1763.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('test_data/mk_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df_test_mace =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_mace[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-mk-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())\n",
    "\n",
    "df_test_mace[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d34e93-66e2-4576-808d-e46b8f952112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = pd.concat([df_test_eng_full,df_test_arab,df_test_bulg,df_test_mace],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cfdec47-1cf5-4c35-986e-fd3f38ebf71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>image_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicola Sturgeon\\n\\nWE'RE SCOTTISH GETUSOUTOFHE...</td>\n",
       "      <td>nicola sturgeon\\n\\nwe're scottish getusoutofhe...</td>\n",
       "      <td>9</td>\n",
       "      <td>68254</td>\n",
       "      <td>prop_meme_8883.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw a movie once where only the police and m...</td>\n",
       "      <td>i saw a movie once where only the police and m...</td>\n",
       "      <td>9</td>\n",
       "      <td>69640</td>\n",
       "      <td>prop_meme_10818.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heaven has a Wall and strict immigration polic...</td>\n",
       "      <td>heaven has a wall and strict immigration polic...</td>\n",
       "      <td>9</td>\n",
       "      <td>71251</td>\n",
       "      <td>prop_meme_7048.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don't expect a broken government to fix itself.</td>\n",
       "      <td>don't expect a broken government to fix itself.</td>\n",
       "      <td>9</td>\n",
       "      <td>79369</td>\n",
       "      <td>prop_meme_15611.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOW MOST AMERICANS SEE THE DEBATE\\n\\nHOW FREE ...</td>\n",
       "      <td>how most americans see the debate\\n\\nhow free ...</td>\n",
       "      <td>9</td>\n",
       "      <td>69351</td>\n",
       "      <td>prop_meme_10392.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>Невакцинираните\\nсе потенцијални\\nсамоубијци и...</td>\n",
       "      <td>invalids are potential suicides and will not e...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_319</td>\n",
       "      <td>mk_memes_319.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>И ВО ВТОРАТА СВЕТСКА ВОЈНА ИМАШЕ НАШИ ШТО ОДЕА...</td>\n",
       "      <td>and in the second world war, ours was what wen...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_321</td>\n",
       "      <td>mk_memes_321.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>Методија Андонов\\nЧенто бил калкулант.\\nДури о...</td>\n",
       "      <td>methodiusa andonov chento was a calculator, on...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_323</td>\n",
       "      <td>mk_memes_323.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>НЕМА ВЕЌЕ ДАТУМ\\n\\n</td>\n",
       "      <td>no more date</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_324</td>\n",
       "      <td>mk_memes_324.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>Волкот од шумата\\n\\nАко Бугарија излезеше од Е...</td>\n",
       "      <td>a wolf from the forest if bulgaria had come ou...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_325</td>\n",
       "      <td>mk_memes_325.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2315 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Nicola Sturgeon\\n\\nWE'RE SCOTTISH GETUSOUTOFHE...   \n",
       "1     I saw a movie once where only the police and m...   \n",
       "2     Heaven has a Wall and strict immigration polic...   \n",
       "3       Don't expect a broken government to fix itself.   \n",
       "4     HOW MOST AMERICANS SEE THE DEBATE\\n\\nHOW FREE ...   \n",
       "...                                                 ...   \n",
       "2310  Невакцинираните\\nсе потенцијални\\nсамоубијци и...   \n",
       "2311  И ВО ВТОРАТА СВЕТСКА ВОЈНА ИМАШЕ НАШИ ШТО ОДЕА...   \n",
       "2312  Методија Андонов\\nЧенто бил калкулант.\\nДури о...   \n",
       "2313                                НЕМА ВЕЌЕ ДАТУМ\\n\\n   \n",
       "2314  Волкот од шумата\\n\\nАко Бугарија излезеше од Е...   \n",
       "\n",
       "                                               aug_text  label            id  \\\n",
       "0     nicola sturgeon\\n\\nwe're scottish getusoutofhe...      9         68254   \n",
       "1     i saw a movie once where only the police and m...      9         69640   \n",
       "2     heaven has a wall and strict immigration polic...      9         71251   \n",
       "3       don't expect a broken government to fix itself.      9         79369   \n",
       "4     how most americans see the debate\\n\\nhow free ...      9         69351   \n",
       "...                                                 ...    ...           ...   \n",
       "2310  invalids are potential suicides and will not e...      9  mk_memes_319   \n",
       "2311  and in the second world war, ours was what wen...      9  mk_memes_321   \n",
       "2312  methodiusa andonov chento was a calculator, on...      9  mk_memes_323   \n",
       "2313                                       no more date      9  mk_memes_324   \n",
       "2314  a wolf from the forest if bulgaria had come ou...      9  mk_memes_325   \n",
       "\n",
       "                 image_ID  \n",
       "0      prop_meme_8883.png  \n",
       "1     prop_meme_10818.png  \n",
       "2      prop_meme_7048.png  \n",
       "3     prop_meme_15611.png  \n",
       "4     prop_meme_10392.png  \n",
       "...                   ...  \n",
       "2310     mk_memes_319.png  \n",
       "2311     mk_memes_321.png  \n",
       "2312     mk_memes_323.png  \n",
       "2313     mk_memes_324.png  \n",
       "2314     mk_memes_325.png  \n",
       "\n",
       "[2315 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_full\n",
    "df_test_full.to_csv('Merged_test_for_HypEmo_2b.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
