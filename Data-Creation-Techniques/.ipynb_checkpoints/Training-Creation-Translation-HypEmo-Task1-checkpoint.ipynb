{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fc4f36-1be8-4e70-aba9-fbb0ea186624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "## Training set preperation\n",
    "dict_mapping = {'Presenting Irrelevant Data (Red Herring)': 0,\n",
    " 'Bandwagon': 1,\n",
    " 'Smears': 2,\n",
    " 'Glittering generalities (Virtue)': 3,\n",
    " 'Causal Oversimplification': 4,\n",
    " 'Whataboutism': 5,\n",
    " 'Loaded Language': 6,\n",
    " 'Exaggeration/Minimisation': 7,\n",
    " 'Repetition': 8,\n",
    " 'Thought-terminating cliché': 9,\n",
    " 'Name calling/Labeling': 10,\n",
    " 'Appeal to authority': 11,\n",
    " 'Black-and-white Fallacy/Dictatorship': 12,\n",
    " 'Obfuscation, Intentional vagueness, Confusion': 13,\n",
    " 'Reductio ad hitlerum': 14,\n",
    " 'Appeal to fear/prejudice': 15,\n",
    " \"Misrepresentation of Someone's Position (Straw Man)\": 16,\n",
    " 'Flag-waving': 17,\n",
    " 'Slogans': 18,\n",
    " 'Doubt': 19,\n",
    " 'Bandwagon_1': 20,\n",
    " 'Whataboutism_1': 21,\n",
    " 'Appeal to fear/prejudice_1': 22,\n",
    " 'Flag-waving_1': 23,\n",
    " 'Appeal to authority_1': 24,\n",
    "}\n",
    "\n",
    "\n",
    "def dataset_f(data,dict_mapping):\n",
    "    set_x = set()\n",
    "    for element in data:\n",
    "        for label in element['labels']:\n",
    "            set_x.add(label)\n",
    "    \n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"labels\"] = element[\"labels\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        if len(element['labels']):\n",
    "            data_id.append(dict_x)\n",
    "    \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    for element in data_id:\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        pattern = r'\\\\n'\n",
    "        cleaned_text = re.sub(pattern, ' ',cleaned_text)\n",
    "        pattern = r'[^a-zA-Z0-9\\s]+'\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text).lower()\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        if(cleaned_text != ''):\n",
    "            unprocessed.append(element['text'])\n",
    "            processed.append(cleaned_text)\n",
    "            labels.append(element['labels'])\n",
    "            ID.append(element['id'])\n",
    "        \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        l_labels = row['label']\n",
    "        empty_list = []\n",
    "        for label in l_labels:\n",
    "            empty_list.append(dict_mapping[label])\n",
    "        row['label'] = empty_list\n",
    "\n",
    "    return df\n",
    "    \n",
    "with open(r'data_given_to_work_with/subtask1/train.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_train = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'data_given_to_work_with/subtask1/validation.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_val = dataset_f(data,dict_mapping)\n",
    "\n",
    "\n",
    "with open(r'data_given_to_work_with/subtask1/training_set_task1.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_extra = dataset_f(data,dict_mapping)\n",
    "\n",
    "\n",
    "df_train_full = pd.concat([df_train,df_val,df_extra],axis=0)\n",
    "df_exploded_train = df_train_full.explode('label')\n",
    "\n",
    "condition = df_exploded_train['label'].isin([1, 5, 15, 17, 11])\n",
    "df_duplicated = df_exploded_train[condition].copy()\n",
    "mapping_dict = {1: 20, 5: 21, 15: 22, 17: 23, 11: 24}\n",
    "df_duplicated['label'] = df_duplicated['label'].map(mapping_dict)\n",
    "df_duplicated = df_duplicated.drop_duplicates()\n",
    "result_df = pd.concat([df_exploded_train, df_duplicated], ignore_index=True)\n",
    "# result_df.to_csv('Merged_train_for_HypEmo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb126af-a965-42b8-a5ef-f49d79ade842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "def dataset_f(data,dict_mapping):\n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        data_id.append(dict_x)\n",
    "        \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    \n",
    "    print(len(data_id))\n",
    "    for element in data_id:\n",
    "        unprocessed.append(element['text'])\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        processed.append(cleaned_text)\n",
    "        x = 9\n",
    "        labels.append(x)\n",
    "        ID.append(element['id'])\n",
    "    \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "with open(r'data_given_to_work_with/subtask1/dev_subtask1_en.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_dev_eng_full = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'data_given_to_work_with/subtask1/en_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_eng_full = dataset_f(data,dict_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2285bf0-3860-41b5-b252-731d8f68051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:19<00:00,  2.83s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1244.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('Files_for_task1a_input/ar_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_arab =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_arab[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True))    \n",
    "df_test_arab[\"aug_text\"] = t_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cc16571-c447-4cd0-ad7e-bfe00f59104b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 25%|████████████████████▊                                                              | 7/28 [02:04<06:13, 17.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     batch_src_text \u001b[38;5;241m=\u001b[39m src_text[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     19\u001b[0m     batch_src_text \u001b[38;5;241m=\u001b[39m tokenizer(batch_src_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 20\u001b[0m     batch_translated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_src_text)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     translations\u001b[38;5;241m.\u001b[39mextend(batch_translated)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(translations):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1611\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1605\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1606\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1607\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1608\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1609\u001b[0m     )\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[0;32m   1612\u001b[0m         input_ids,\n\u001b[0;32m   1613\u001b[0m         beam_scorer,\n\u001b[0;32m   1614\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1615\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1616\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1617\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1618\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1619\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1620\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1622\u001b[0m     )\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:2928\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2923\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust_logits_during_generation(next_token_logits, cur_len\u001b[38;5;241m=\u001b[39mcur_len)\n\u001b[0;32m   2924\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[0;32m   2925\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2926\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 2928\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2929\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(next_token_scores)\n\u001b[0;32m   2931\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\logits_process.py:593\u001b[0m, in \u001b[0;36mNoBadWordsLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_bad_words_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbad_words_id_length_1) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_bad_words_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_static_bad_word_mask(scores)\n\u001b[1;32m--> 593\u001b[0m dynamic_banned_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_banned_bad_words_ids(\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    594\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_scores_to_inf_for_banned_tokens(scores, dynamic_banned_tokens)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('Files_for_task1a_input/bg_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_bulg =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_bulg[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-bg-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())    \n",
    "df_test_bulg[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849bb03-0cf7-47c0-a7ba-a118da896277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('Files_for_task1a_input/mk_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df_test_mace =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_mace[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-mk-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())\n",
    "\n",
    "df_test_mace[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e520c8a3-62a2-4401-aa29-87518142cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_bulg = pd.read_csv('bulg.csv').drop('Unnamed: 0',axis=1)\n",
    "df_test_mace = pd.read_csv('mace.csv').drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90a8a36f-d4fe-4e56-9ba5-72685fc0aa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ДНЕВНИК\\n\\nКАПИТАЛ\\n\\nAMERICA FOR BULGARIA FOU...</td>\n",
       "      <td>derivative tier 1 america for bulgaria foundat...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Когато съм се ваксинирал и се върна на село да...</td>\n",
       "      <td>when i was vaccinated and returned to the vill...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ганя, ваксинирай се, глупако!!\\n\\nГаня, полудя...</td>\n",
       "      <td>ganya, you're inoculating yourself, you idiot!</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Не можеш ли да работиш нещо нормално като друг...</td>\n",
       "      <td>can't you work something normal like other peo...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Не назеления сертификат\\n\\nВаксиниран съм\\nПри...</td>\n",
       "      <td>no green certificate i've been vaccinated i've...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>@MEME PARTIQ\\nМакедонска\\nСветовна\\nистория\\nи...</td>\n",
       "      <td>@mememe partiq macedonian world history</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Никога не е била!\\nЧакай, Земята не\\nе македон...</td>\n",
       "      <td>wait, the earth isn't macedonian?</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Тате Тате || даа\\nАз съм\\nМакедонфц\\nthe\\nЩе г...</td>\n",
       "      <td>dad, i'm the macedonfz, you're gonna burn at y...</td>\n",
       "      <td>9</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>МАКЕДОНИЯ и\\nБЪЛГАРИЯ\\nМАКЕДОНИЯ и\\n- БЪЛГАРИЯ...</td>\n",
       "      <td>macedonia and bulgaria macedonia and - bulgari...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>АМЕРИКАНСКАТА МЕЧТА\\nglasspar\\nDoughboy\\nCHATI...</td>\n",
       "      <td>american bears glasspar doughboy chaties chiem...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ДНЕВНИК\\n\\nКАПИТАЛ\\n\\nAMERICA FOR BULGARIA FOU...   \n",
       "1    Когато съм се ваксинирал и се върна на село да...   \n",
       "2    Ганя, ваксинирай се, глупако!!\\n\\nГаня, полудя...   \n",
       "3    Не можеш ли да работиш нещо нормално като друг...   \n",
       "4    Не назеления сертификат\\n\\nВаксиниран съм\\nПри...   \n",
       "..                                                 ...   \n",
       "431  @MEME PARTIQ\\nМакедонска\\nСветовна\\nистория\\nи...   \n",
       "432  Никога не е била!\\nЧакай, Земята не\\nе македон...   \n",
       "433  Тате Тате || даа\\nАз съм\\nМакедонфц\\nthe\\nЩе г...   \n",
       "434  МАКЕДОНИЯ и\\nБЪЛГАРИЯ\\nМАКЕДОНИЯ и\\n- БЪЛГАРИЯ...   \n",
       "435  АМЕРИКАНСКАТА МЕЧТА\\nglasspar\\nDoughboy\\nCHATI...   \n",
       "\n",
       "                                              aug_text  label               id  \n",
       "0    derivative tier 1 america for bulgaria foundat...      9   bg_mk_memes_83  \n",
       "1    when i was vaccinated and returned to the vill...      9  bg_mk_memes_104  \n",
       "2       ganya, you're inoculating yourself, you idiot!      9  bg_mk_memes_106  \n",
       "3    can't you work something normal like other peo...      9  bg_mk_memes_148  \n",
       "4    no green certificate i've been vaccinated i've...      9  bg_mk_memes_237  \n",
       "..                                                 ...    ...              ...  \n",
       "431            @mememe partiq macedonian world history      9               74  \n",
       "432                  wait, the earth isn't macedonian?      9               75  \n",
       "433  dad, i'm the macedonfz, you're gonna burn at y...      9               76  \n",
       "434  macedonia and bulgaria macedonia and - bulgari...      9                8  \n",
       "435  american bears glasspar doughboy chaties chiem...      9                9  \n",
       "\n",
       "[436 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_bulg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06d34e93-66e2-4576-808d-e46b8f952112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = pd.concat([df_dev_eng_full,df_test_eng_full,df_test_arab,df_test_bulg,df_test_mace],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cfdec47-1cf5-4c35-986e-fd3f38ebf71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is why we're free\\n\\nThis is why we're sa...</td>\n",
       "      <td>this is why we're free\\n\\nthis is why we're sa...</td>\n",
       "      <td>9</td>\n",
       "      <td>63292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IF YOU SAY WE'RE IN THE MIDDLE OF A DEADLY PAN...</td>\n",
       "      <td>if you say we're in the middle of a deadly pan...</td>\n",
       "      <td>9</td>\n",
       "      <td>70419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFTER A YEAR OF ZERO EVIDENCE OF RUSSIAN COLLU...</td>\n",
       "      <td>after a year of zero evidence of russian collu...</td>\n",
       "      <td>9</td>\n",
       "      <td>63673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MY WAY MY WAY</td>\n",
       "      <td>my way my way</td>\n",
       "      <td>9</td>\n",
       "      <td>71297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Putin signed a decree to exclude Lyman from Ru...</td>\n",
       "      <td>putin signed a decree to exclude lyman from ru...</td>\n",
       "      <td>9</td>\n",
       "      <td>66340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>Невакцинираните\\nсе потенцијални\\nсамоубијци и...</td>\n",
       "      <td>invalids are potential suicides and will not e...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>И ВО ВТОРАТА СВЕТСКА ВОЈНА ИМАШЕ НАШИ ШТО ОДЕА...</td>\n",
       "      <td>and in the second world war, ours was what wen...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>Методија Андонов\\nЧенто бил калкулант.\\nДури о...</td>\n",
       "      <td>methodiusa andonov chento was a calculator, on...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>НЕМА ВЕЌЕ ДАТУМ\\n\\n</td>\n",
       "      <td>no more date</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>Волкот од шумата\\n\\nАко Бугарија излезеше од Е...</td>\n",
       "      <td>a wolf from the forest if bulgaria had come ou...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3295 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     This is why we're free\\n\\nThis is why we're sa...   \n",
       "1     IF YOU SAY WE'RE IN THE MIDDLE OF A DEADLY PAN...   \n",
       "2     AFTER A YEAR OF ZERO EVIDENCE OF RUSSIAN COLLU...   \n",
       "3                                         MY WAY MY WAY   \n",
       "4     Putin signed a decree to exclude Lyman from Ru...   \n",
       "...                                                 ...   \n",
       "3290  Невакцинираните\\nсе потенцијални\\nсамоубијци и...   \n",
       "3291  И ВО ВТОРАТА СВЕТСКА ВОЈНА ИМАШЕ НАШИ ШТО ОДЕА...   \n",
       "3292  Методија Андонов\\nЧенто бил калкулант.\\nДури о...   \n",
       "3293                                НЕМА ВЕЌЕ ДАТУМ\\n\\n   \n",
       "3294  Волкот од шумата\\n\\nАко Бугарија излезеше од Е...   \n",
       "\n",
       "                                               aug_text  label            id  \n",
       "0     this is why we're free\\n\\nthis is why we're sa...      9         63292  \n",
       "1     if you say we're in the middle of a deadly pan...      9         70419  \n",
       "2     after a year of zero evidence of russian collu...      9         63673  \n",
       "3                                         my way my way      9         71297  \n",
       "4     putin signed a decree to exclude lyman from ru...      9         66340  \n",
       "...                                                 ...    ...           ...  \n",
       "3290  invalids are potential suicides and will not e...      9  mk_memes_319  \n",
       "3291  and in the second world war, ours was what wen...      9  mk_memes_321  \n",
       "3292  methodiusa andonov chento was a calculator, on...      9  mk_memes_323  \n",
       "3293                                       no more date      9  mk_memes_324  \n",
       "3294  a wolf from the forest if bulgaria had come ou...      9  mk_memes_325  \n",
       "\n",
       "[3295 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_full\n",
    "#1000 + 1500 + 100 + 436 + 259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9aeeac44-aaad-46ef-b845-ebc5bad84f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_eng_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41010ac6-f876-4069-b379-5f9bb9245cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test_full = pd.read_csv('Merged_test_for_HypEmo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04c74420-ef22-4682-a769-79af3b43c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the .npy file\n",
    "loaded_array = np.load('softmax_0_allcomeon_test_finally.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67373ef8-3b46-4ed4-8e2e-2e55a97341a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loaded_array = np.load('softmax_9_test_arey.npy')\n",
    "mapping_dict = {1: 20, 5: 21, 15: 22, 17: 23, 11: 24}\n",
    "for key in mapping_dict.keys():\n",
    "    duplicate_row = mapping_dict[key]\n",
    "    loaded_array[key] += loaded_array[int(duplicate_row)]\n",
    "loaded_array = loaded_array[:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17924d8d-9af1-4849-9964-d2eafb7b2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "abnormal_peaks_per_row = []\n",
    "# index = 13\n",
    "threshold = 1.5\n",
    "for row in loaded_array:\n",
    "    # Calculate Z-scores\n",
    "    z_scores = np.abs((row - np.mean(row)) / np.std(row))\n",
    "    # Identify abnormal peaks\n",
    "    abnormal_peaks = list(np.where(z_scores > threshold)[0])\n",
    "    # Store abnormal peaks for the current row\n",
    "    abnormal_peaks_per_row.append(abnormal_peaks)\n",
    "    # plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "    # plt.plot(loaded_array[index])\n",
    "    # plt.plot(z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c42f36-04a6-4355-8119-ca5727c22746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3295"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abnormal_peaks_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5472916-4166-4420-bb34-3af24a548e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "idx2label = {idx: label for label, idx in dict_mapping.items()}\n",
    "for x in abnormal_peaks_per_row:\n",
    "    y = []\n",
    "    for d in x:\n",
    "        y.append(idx2label[d])\n",
    "    final_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4157cd99-10c7-4a9a-8326-98ceb4110fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3295"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538edbea-fcf1-4a62-9606-9171640271dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full['labels'] = final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537ad6f7-80e2-422b-86c0-27245c0216b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is why we're free\\n\\nThis is why we're sa...</td>\n",
       "      <td>this is why we're free\\n\\nthis is why we're sa...</td>\n",
       "      <td>9</td>\n",
       "      <td>63292</td>\n",
       "      <td>[Repetition]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IF YOU SAY WE'RE IN THE MIDDLE OF A DEADLY PAN...</td>\n",
       "      <td>if you say we're in the middle of a deadly pan...</td>\n",
       "      <td>9</td>\n",
       "      <td>70419</td>\n",
       "      <td>[Loaded Language, Name calling/Labeling, Slogans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AFTER A YEAR OF ZERO EVIDENCE OF RUSSIAN COLLU...</td>\n",
       "      <td>after a year of zero evidence of russian collu...</td>\n",
       "      <td>9</td>\n",
       "      <td>63673</td>\n",
       "      <td>[Smears, Loaded Language]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MY WAY MY WAY</td>\n",
       "      <td>my way my way</td>\n",
       "      <td>9</td>\n",
       "      <td>71297</td>\n",
       "      <td>[Black-and-white Fallacy/Dictatorship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Putin signed a decree to exclude Lyman from Ru...</td>\n",
       "      <td>putin signed a decree to exclude lyman from ru...</td>\n",
       "      <td>9</td>\n",
       "      <td>66340</td>\n",
       "      <td>[Smears]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>3290</td>\n",
       "      <td>Невакцинираните\\nсе потенцијални\\nсамоубијци и...</td>\n",
       "      <td>invalids are potential suicides and will not e...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_319</td>\n",
       "      <td>[Appeal to authority]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>3291</td>\n",
       "      <td>И ВО ВТОРАТА СВЕТСКА ВОЈНА ИМАШЕ НАШИ ШТО ОДЕА...</td>\n",
       "      <td>and in the second world war, ours was what wen...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_321</td>\n",
       "      <td>[Smears, Loaded Language, Name calling/Labeling]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>3292</td>\n",
       "      <td>Методија Андонов\\nЧенто бил калкулант.\\nДури о...</td>\n",
       "      <td>methodiusa andonov chento was a calculator, on...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_323</td>\n",
       "      <td>[Smears, Glittering generalities (Virtue), Loa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>3293</td>\n",
       "      <td>НЕМА ВЕЌЕ ДАТУМ\\n\\n</td>\n",
       "      <td>no more date</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_324</td>\n",
       "      <td>[Black-and-white Fallacy/Dictatorship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>3294</td>\n",
       "      <td>Волкот од шумата\\n\\nАко Бугарија излезеше од Е...</td>\n",
       "      <td>a wolf from the forest if bulgaria had come ou...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_325</td>\n",
       "      <td>[Loaded Language]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3295 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  This is why we're free\\n\\nThis is why we're sa...   \n",
       "1              1  IF YOU SAY WE'RE IN THE MIDDLE OF A DEADLY PAN...   \n",
       "2              2  AFTER A YEAR OF ZERO EVIDENCE OF RUSSIAN COLLU...   \n",
       "3              3                                      MY WAY MY WAY   \n",
       "4              4  Putin signed a decree to exclude Lyman from Ru...   \n",
       "...          ...                                                ...   \n",
       "3290        3290  Невакцинираните\\nсе потенцијални\\nсамоубијци и...   \n",
       "3291        3291  И ВО ВТОРАТА СВЕТСКА ВОЈНА ИМАШЕ НАШИ ШТО ОДЕА...   \n",
       "3292        3292  Методија Андонов\\nЧенто бил калкулант.\\nДури о...   \n",
       "3293        3293                                НЕМА ВЕЌЕ ДАТУМ\\n\\n   \n",
       "3294        3294  Волкот од шумата\\n\\nАко Бугарија излезеше од Е...   \n",
       "\n",
       "                                               aug_text  label            id  \\\n",
       "0     this is why we're free\\n\\nthis is why we're sa...      9         63292   \n",
       "1     if you say we're in the middle of a deadly pan...      9         70419   \n",
       "2     after a year of zero evidence of russian collu...      9         63673   \n",
       "3                                         my way my way      9         71297   \n",
       "4     putin signed a decree to exclude lyman from ru...      9         66340   \n",
       "...                                                 ...    ...           ...   \n",
       "3290  invalids are potential suicides and will not e...      9  mk_memes_319   \n",
       "3291  and in the second world war, ours was what wen...      9  mk_memes_321   \n",
       "3292  methodiusa andonov chento was a calculator, on...      9  mk_memes_323   \n",
       "3293                                       no more date      9  mk_memes_324   \n",
       "3294  a wolf from the forest if bulgaria had come ou...      9  mk_memes_325   \n",
       "\n",
       "                                                 labels  \n",
       "0                                          [Repetition]  \n",
       "1     [Loaded Language, Name calling/Labeling, Slogans]  \n",
       "2                             [Smears, Loaded Language]  \n",
       "3                [Black-and-white Fallacy/Dictatorship]  \n",
       "4                                              [Smears]  \n",
       "...                                                 ...  \n",
       "3290                              [Appeal to authority]  \n",
       "3291   [Smears, Loaded Language, Name calling/Labeling]  \n",
       "3292  [Smears, Glittering generalities (Virtue), Loa...  \n",
       "3293             [Black-and-white Fallacy/Dictatorship]  \n",
       "3294                                  [Loaded Language]  \n",
       "\n",
       "[3295 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "849cd8bb-0c52-42e3-9d60-c012a0109164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data has been saved to: Hyp_emo_test_eng.json\n",
      "JSON data has been saved to: Hyp_emo_test_arab.json\n",
      "JSON data has been saved to: Hyp_emo_test_bulg.json\n",
      "JSON data has been saved to: Hyp_emo_test_mace.json\n"
     ]
    }
   ],
   "source": [
    "# dev_eng = df_test_full[:1000]\n",
    "test_eng = df_test_full[1000:1000+1500]\n",
    "test_arab = df_test_full[1000+1500:1000+1500+100]\n",
    "test_bulg = df_test_full[1000+1500+100:1000+1500+100+436]\n",
    "test_mace = df_test_full[1000+1500+100+436:]\n",
    "#1000 + 1500 + 100 + 436 + 259\n",
    "\n",
    "def json_saving(df,json_file_path):\n",
    "    dict_list = []\n",
    "    columns_to_include = ['labels', 'id']\n",
    "    for index, row in df.iterrows():    \n",
    "        row_dict = {column: row[column] for column in columns_to_include}\n",
    "        dict_list.append(row_dict)\n",
    "    json_string = json.dumps(dict_list,indent = 2)\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json_file.write(json_string)\n",
    "    print(f'JSON data has been saved to: {json_file_path}')\n",
    "\n",
    "# json_saving(dev_eng,\"Hyp_emo_dev_eng.json\")\n",
    "json_saving(test_eng,\"Hyp_emo_test_eng.json\")\n",
    "json_saving(test_arab,\"Hyp_emo_test_arab.json\")\n",
    "json_saving(test_bulg,\"Hyp_emo_test_bulg.json\")\n",
    "json_saving(test_mace,\"Hyp_emo_test_mace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ebd05a83-7ff7-455d-be82-fd4f704731aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.53597\tprec_h=0.55005\trec_h=0.52258\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g dev_subtask1_en.json -p BERT_preds_on_dev_task1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb5381ee-f2e5-4d8e-905d-cab3c49f617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.56426\tprec_h=0.58179\trec_h=0.54776\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g dev_subtask1_en.json -p Hyp_emo_dev_eng.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "691235b1-d384-45ec-a9e0-a8dd7c02fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.14444\tprec_h=0.15294\trec_h=0.13684\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g test_subtask1_ar.json -p Hyp_emo_test_arab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e7f2e086-b0c8-43ff-8714-4b8422386da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.31589\tprec_h=0.31360\trec_h=0.31821\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g test_subtask1_bg.json -p Hyp_emo_test_bulg.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "71d70334-78b9-4c94-aa66-4cc93305f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.24791\tprec_h=0.24046\trec_h=0.25584\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g test_subtask1_md.json -p Hyp_emo_test_mace.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744197f3-5261-4a97-a812-979daf5cf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "columns_to_include = ['labels', 'id']\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in df_test_full.iterrows():\n",
    "    # Create a dictionary for the current row with only specified columns\n",
    "    row_dict = {column: row[column] for column in columns_to_include}\n",
    "    # Append the dictionary to the list\n",
    "    dict_list.append(row_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
