{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fc4f36-1be8-4e70-aba9-fbb0ea186624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "## Training set preperation\n",
    "dict_mapping = {'Presenting Irrelevant Data (Red Herring)': 0,\n",
    " 'Bandwagon': 1,\n",
    " 'Smears': 2,\n",
    " 'Glittering generalities (Virtue)': 3,\n",
    " 'Causal Oversimplification': 4,\n",
    " 'Whataboutism': 5,\n",
    " 'Loaded Language': 6,\n",
    " 'Exaggeration/Minimisation': 7,\n",
    " 'Repetition': 8,\n",
    " 'Thought-terminating clich√©': 9,\n",
    " 'Name calling/Labeling': 10,\n",
    " 'Appeal to authority': 11,\n",
    " 'Black-and-white Fallacy/Dictatorship': 12,\n",
    " 'Obfuscation, Intentional vagueness, Confusion': 13,\n",
    " 'Reductio ad hitlerum': 14,\n",
    " 'Appeal to fear/prejudice': 15,\n",
    " \"Misrepresentation of Someone's Position (Straw Man)\": 16,\n",
    " 'Flag-waving': 17,\n",
    " 'Slogans': 18,\n",
    " 'Doubt': 19,\n",
    " 'Bandwagon_1': 20,\n",
    " 'Whataboutism_1': 21,\n",
    " 'Appeal to fear/prejudice_1': 22,\n",
    " 'Flag-waving_1': 23,\n",
    " 'Appeal to authority_1': 24,\n",
    " \"Transfer_1\" : 25,\n",
    " \"Transfer\" : 26,\n",
    " \"Appeal to (Strong) Emotions\" : 27\n",
    "}\n",
    "\n",
    "\n",
    "def dataset_f(data,dict_mapping):\n",
    "    set_x = set()\n",
    "    for element in data:\n",
    "        for label in element['labels']:\n",
    "            set_x.add(label)\n",
    "    \n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"labels\"] = element[\"labels\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        dict_x[\"image\"] = element[\"image\"]\n",
    "        if len(element['labels']):\n",
    "            data_id.append(dict_x)\n",
    "    \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    image_ID = []\n",
    "    for element in data_id:\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        pattern = r'\\\\n'\n",
    "        cleaned_text = re.sub(pattern, ' ',cleaned_text)\n",
    "        pattern = r'[^a-zA-Z0-9\\s]+'\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text).lower()\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        if(cleaned_text != ''):\n",
    "            unprocessed.append(element['text'])\n",
    "            processed.append(cleaned_text)\n",
    "            labels.append(element['labels'])\n",
    "            ID.append(element['id'])\n",
    "            image_ID.append(element['image'])\n",
    "            \n",
    "        \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID,'image_ID' : image_ID}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        l_labels = row['label']\n",
    "        empty_list = []\n",
    "        for label in l_labels:\n",
    "            empty_list.append(dict_mapping[label])\n",
    "        row['label'] = empty_list\n",
    "\n",
    "    return df\n",
    "    \n",
    "with open(r'train.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_train = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'validation.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_val = dataset_f(data,dict_mapping)\n",
    "\n",
    "\n",
    "df_train_full = pd.concat([df_train,df_val],axis=0)\n",
    "df_exploded_train = df_train_full.explode('label')\n",
    "\n",
    "condition = df_exploded_train['label'].isin([1, 5, 15, 17, 11, 26])\n",
    "df_duplicated = df_exploded_train[condition].copy()\n",
    "mapping_dict = {1: 20, 5: 21, 15: 22, 17: 23, 11: 24,26 : 25}\n",
    "df_duplicated['label'] = df_duplicated['label'].map(mapping_dict)\n",
    "df_duplicated = df_duplicated.drop_duplicates()\n",
    "result_df = pd.concat([df_exploded_train, df_duplicated], ignore_index=True)\n",
    "result_df.to_csv('Merged_train_for_HypEmo_2b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb126af-a965-42b8-a5ef-f49d79ade842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dataset_f(data,dict_mapping):\n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        dict_x[\"image\"] = element[\"image\"]\n",
    "        data_id.append(dict_x)\n",
    "        \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    image_ID = []\n",
    "    \n",
    "    print(len(data_id))\n",
    "    for element in data_id:\n",
    "        unprocessed.append(element['text'])\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        processed.append(cleaned_text)\n",
    "        x = 9\n",
    "        labels.append(x)\n",
    "        ID.append(element['id'])\n",
    "        image_ID.append(element['image'])\n",
    "    \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID,'image_ID' : image_ID}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# with open(r'data_given_to_work_with/subtask1/dev_subtask1_en.json', 'rb') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "# df_dev_eng_full = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'test_data/en_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_eng_full = dataset_f(data,dict_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2285bf0-3860-41b5-b252-731d8f68051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:25<00:00,  3.22s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [00:00<00:00, 1249.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('test_data/ar_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_arab =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_arab[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True))    \n",
    "df_test_arab[\"aug_text\"] = t_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32fe558-fe08-49bd-8cb0-1ff09671cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>image_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ŸÖÿ¥ÿßŸäÿÆ ÿßŸÑÿ£ŸàŸÇÿßŸÅ ŸàÿßŸÇŸÅŸäŸÜ ŸÑÿ≠ŸÖÿßŸäÿ© ÿßŸÑŸÖÿ≥ÿ¨ÿØ\\nÿ≠ŸÖÿßŸäÿ© ÿßŸÑŸÖÿ≥...</td>\n",
       "      <td>The Waqf Chiefs are standing to protect the mo...</td>\n",
       "      <td>9</td>\n",
       "      <td>00001</td>\n",
       "      <td>prop_meme_00001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÿßŸÑŸÖÿ≥ÿ§ŸàŸÑŸäŸÜ ÿßŸÑÿπÿ±ÿ® ÿ®ÿπÿØŸÖÿß ŸäÿÆÿ±Ÿà ÿπŸÑŸâ ÿ¥ÿπÿ®ŸáŸÖ Ÿà ÿ≠ÿßŸÜ ÿßŸÑÿ¢...</td>\n",
       "      <td>Arab officials, after they've fallen on their ...</td>\n",
       "      <td>9</td>\n",
       "      <td>00002</td>\n",
       "      <td>prop_meme_00002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÿπŸÜÿØŸÖÿß Ÿäÿ≥ÿ£ŸÑ ÿ®Ÿàÿ™ŸÅŸÑŸäŸÇÿ© ÿπŸÜ ÿßŸÑŸÖÿØÿ© ÿßŸÑÿ™Ÿä Ÿäÿ±ŸäÿØ ŸÇÿ∂ÿßÿ°Ÿáÿß ...</td>\n",
       "      <td>When Pottflika asks how long Renessa wants to ...</td>\n",
       "      <td>9</td>\n",
       "      <td>00003</td>\n",
       "      <td>prop_meme_00003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ŸÑÿß ŸäÿπŸÜŸä ÿßŸÜŸá ÿßÿ∞ÿß ÿ≠ÿ∑Ÿäÿ™ ÿßŸäÿØŸä ÿπŸÑŸäŸÉŸä ÿßŸà ÿßÿ™ÿ∑ŸÑÿπÿ™ ÿπ ÿµÿØ...</td>\n",
       "      <td>Doesn't mean that if I put Eddie on you or loo...</td>\n",
       "      <td>9</td>\n",
       "      <td>00004</td>\n",
       "      <td>prop_meme_00004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*ÿßÿ≥ÿ±ÿßÿ¶ŸäŸÑ*\\n*ŸÇÿ™ŸÑ ŸàŸáÿØŸÖ Ÿàÿßÿπÿ™ŸÇÿßŸÑÿßÿ™\\n* ŸÖÿ≥ÿ™Ÿàÿ∑ŸÜÿßÿ™\\n*ÿ∂...</td>\n",
       "      <td>Israel's killing, demolition and arrests of se...</td>\n",
       "      <td>9</td>\n",
       "      <td>00005</td>\n",
       "      <td>prop_meme_00005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Ÿáÿ∞ÿß ÿßŸÑŸÇÿ∑ ÿ≠Ÿäÿßÿ™Ÿá ÿ®ÿßÿ¶ÿ≥Ÿá ÿ¨ÿØÿß ŸàŸÅŸÇŸäÿ±ŸÑÿßŸäŸÖŸÑŸÉ ÿ±ÿ®ÿπ ÿ™ŸàŸÜŸáüíî\\n</td>\n",
       "      <td>This cat's life is so miserable and poor, he d...</td>\n",
       "      <td>9</td>\n",
       "      <td>00499</td>\n",
       "      <td>prop_meme_00499.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>| ŸáŸâ ŸÉŸàÿ±ŸàŸÜÿß ÿØŸâ ÿØÿßÿ¨ŸÑ ŸàŸÑÿß ÿ≥ÿ™\\n ŸàÿßŸÑŸÑŸá Ÿäÿßÿ®ŸÜÿ™Ÿâ ÿ∑ÿßŸÑŸÖ...</td>\n",
       "      <td>It's Corona de Doughle and La Six, and God, my...</td>\n",
       "      <td>9</td>\n",
       "      <td>00506</td>\n",
       "      <td>prop_meme_00506.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>ÿ®ÿ™ÿ∂ÿ±ÿ® ÿ£ÿπÿ≤ ÿ£ÿµÿ≠ÿßÿ®ŸÉ ŸÖŸÇÿßÿ®ŸÑ ŸÖŸÑŸäŸàŸÜ ÿØŸàŸÑÿßÿ±ÿüÿü\\nÿ£ŸÜÿß:</td>\n",
       "      <td>By hitting your best friend for a million doll...</td>\n",
       "      <td>9</td>\n",
       "      <td>00541</td>\n",
       "      <td>prop_meme_00541.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ŸàŸÑÿßÿØŸä ÿ®ÿπÿØ ŸÖÿß ÿ™ŸÜŸÖÿ±ÿ™ ÿπŸÑŸâ ŸÜÿµ ÿßŸÑŸÉŸàŸÉÿ®</td>\n",
       "      <td>My birth after you bullied on the planet's text.</td>\n",
       "      <td>9</td>\n",
       "      <td>00542</td>\n",
       "      <td>prop_meme_00542.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ŸÑŸÖÿß ÿ™ÿ™ÿ£ÿÆÿ± ÿ®ÿßŸÑÿ±ÿ¨ÿπÿ© ÿπÿßŸÑÿ®Ÿäÿ™ Ÿàÿßÿ®ŸàŸÉ ŸäŸÇŸÑŸÉ ŸàŸäŸÜŸÉ ŸÑŸáŸÑÿ£ ...</td>\n",
       "      <td>When you're back home late, your dad picks you...</td>\n",
       "      <td>9</td>\n",
       "      <td>00573</td>\n",
       "      <td>prop_meme_00573.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ŸÖÿ¥ÿßŸäÿÆ ÿßŸÑÿ£ŸàŸÇÿßŸÅ ŸàÿßŸÇŸÅŸäŸÜ ŸÑÿ≠ŸÖÿßŸäÿ© ÿßŸÑŸÖÿ≥ÿ¨ÿØ\\nÿ≠ŸÖÿßŸäÿ© ÿßŸÑŸÖÿ≥...   \n",
       "1    ÿßŸÑŸÖÿ≥ÿ§ŸàŸÑŸäŸÜ ÿßŸÑÿπÿ±ÿ® ÿ®ÿπÿØŸÖÿß ŸäÿÆÿ±Ÿà ÿπŸÑŸâ ÿ¥ÿπÿ®ŸáŸÖ Ÿà ÿ≠ÿßŸÜ ÿßŸÑÿ¢...   \n",
       "2    ÿπŸÜÿØŸÖÿß Ÿäÿ≥ÿ£ŸÑ ÿ®Ÿàÿ™ŸÅŸÑŸäŸÇÿ© ÿπŸÜ ÿßŸÑŸÖÿØÿ© ÿßŸÑÿ™Ÿä Ÿäÿ±ŸäÿØ ŸÇÿ∂ÿßÿ°Ÿáÿß ...   \n",
       "3    ŸÑÿß ŸäÿπŸÜŸä ÿßŸÜŸá ÿßÿ∞ÿß ÿ≠ÿ∑Ÿäÿ™ ÿßŸäÿØŸä ÿπŸÑŸäŸÉŸä ÿßŸà ÿßÿ™ÿ∑ŸÑÿπÿ™ ÿπ ÿµÿØ...   \n",
       "4    *ÿßÿ≥ÿ±ÿßÿ¶ŸäŸÑ*\\n*ŸÇÿ™ŸÑ ŸàŸáÿØŸÖ Ÿàÿßÿπÿ™ŸÇÿßŸÑÿßÿ™\\n* ŸÖÿ≥ÿ™Ÿàÿ∑ŸÜÿßÿ™\\n*ÿ∂...   \n",
       "..                                                 ...   \n",
       "115   Ÿáÿ∞ÿß ÿßŸÑŸÇÿ∑ ÿ≠Ÿäÿßÿ™Ÿá ÿ®ÿßÿ¶ÿ≥Ÿá ÿ¨ÿØÿß ŸàŸÅŸÇŸäÿ±ŸÑÿßŸäŸÖŸÑŸÉ ÿ±ÿ®ÿπ ÿ™ŸàŸÜŸáüíî\\n   \n",
       "116  | ŸáŸâ ŸÉŸàÿ±ŸàŸÜÿß ÿØŸâ ÿØÿßÿ¨ŸÑ ŸàŸÑÿß ÿ≥ÿ™\\n ŸàÿßŸÑŸÑŸá Ÿäÿßÿ®ŸÜÿ™Ÿâ ÿ∑ÿßŸÑŸÖ...   \n",
       "117         ÿ®ÿ™ÿ∂ÿ±ÿ® ÿ£ÿπÿ≤ ÿ£ÿµÿ≠ÿßÿ®ŸÉ ŸÖŸÇÿßÿ®ŸÑ ŸÖŸÑŸäŸàŸÜ ÿØŸàŸÑÿßÿ±ÿüÿü\\nÿ£ŸÜÿß:   \n",
       "118                   ŸàŸÑÿßÿØŸä ÿ®ÿπÿØ ŸÖÿß ÿ™ŸÜŸÖÿ±ÿ™ ÿπŸÑŸâ ŸÜÿµ ÿßŸÑŸÉŸàŸÉÿ®   \n",
       "119  ŸÑŸÖÿß ÿ™ÿ™ÿ£ÿÆÿ± ÿ®ÿßŸÑÿ±ÿ¨ÿπÿ© ÿπÿßŸÑÿ®Ÿäÿ™ Ÿàÿßÿ®ŸàŸÉ ŸäŸÇŸÑŸÉ ŸàŸäŸÜŸÉ ŸÑŸáŸÑÿ£ ...   \n",
       "\n",
       "                                              aug_text  label     id  \\\n",
       "0    The Waqf Chiefs are standing to protect the mo...      9  00001   \n",
       "1    Arab officials, after they've fallen on their ...      9  00002   \n",
       "2    When Pottflika asks how long Renessa wants to ...      9  00003   \n",
       "3    Doesn't mean that if I put Eddie on you or loo...      9  00004   \n",
       "4    Israel's killing, demolition and arrests of se...      9  00005   \n",
       "..                                                 ...    ...    ...   \n",
       "115  This cat's life is so miserable and poor, he d...      9  00499   \n",
       "116  It's Corona de Doughle and La Six, and God, my...      9  00506   \n",
       "117  By hitting your best friend for a million doll...      9  00541   \n",
       "118   My birth after you bullied on the planet's text.      9  00542   \n",
       "119  When you're back home late, your dad picks you...      9  00573   \n",
       "\n",
       "                image_ID  \n",
       "0    prop_meme_00001.jpg  \n",
       "1    prop_meme_00002.jpg  \n",
       "2    prop_meme_00003.jpg  \n",
       "3    prop_meme_00004.jpg  \n",
       "4    prop_meme_00005.jpg  \n",
       "..                   ...  \n",
       "115  prop_meme_00499.jpg  \n",
       "116  prop_meme_00506.jpg  \n",
       "117  prop_meme_00541.jpg  \n",
       "118  prop_meme_00542.jpg  \n",
       "119  prop_meme_00573.jpg  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_arab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc16571-c447-4cd0-ad7e-bfe00f59104b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/55 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [04:22<00:00,  4.77s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436/436 [00:00<00:00, 1113.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('test_data/bg_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_bulg =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_bulg[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-bg-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 8\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())    \n",
    "df_test_bulg[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f609244c-f0ce-4b10-9ec1-e7ec453d6ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>image_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–î–ù–ï–í–ù–ò–ö\\n\\n–ö–ê–ü–ò–¢–ê–õ\\n\\nAMERICA FOR BULGARIA FOU...</td>\n",
       "      <td>derivative tier 1 america for bulgaria foundat...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_83</td>\n",
       "      <td>bg_mk_memes_83.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ö–æ–≥–∞—Ç–æ —Å—ä–º —Å–µ –≤–∞–∫—Å–∏–Ω–∏—Ä–∞–ª –∏ —Å–µ –≤—ä—Ä–Ω–∞ –Ω–∞ —Å–µ–ª–æ –¥–∞...</td>\n",
       "      <td>when i was vaccinated and returned to the vill...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_104</td>\n",
       "      <td>bg_mk_memes_104.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ì–∞–Ω—è, –≤–∞–∫—Å–∏–Ω–∏—Ä–∞–π —Å–µ, –≥–ª—É–ø–∞–∫–æ!!\\n\\n–ì–∞–Ω—è, –ø–æ–ª—É–¥—è...</td>\n",
       "      <td>ganya, you're inoculating yourself, you idiot!</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_106</td>\n",
       "      <td>bg_mk_memes_106.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ù–µ –º–æ–∂–µ—à –ª–∏ –¥–∞ —Ä–∞–±–æ—Ç–∏—à –Ω–µ—â–æ –Ω–æ—Ä–º–∞–ª–Ω–æ –∫–∞—Ç–æ –¥—Ä—É–≥...</td>\n",
       "      <td>can't you work something normal like other peo...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_148</td>\n",
       "      <td>bg_mk_memes_148.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ù–µ –Ω–∞–∑–µ–ª–µ–Ω–∏—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç\\n\\n–í–∞–∫—Å–∏–Ω–∏—Ä–∞–Ω —Å—ä–º\\n–ü—Ä–∏...</td>\n",
       "      <td>no green certificate i've been vaccinated i've...</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_mk_memes_237</td>\n",
       "      <td>bg_mk_memes_237.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>@MEME PARTIQ\\n–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∞\\n–°–≤–µ—Ç–æ–≤–Ω–∞\\n–∏—Å—Ç–æ—Ä–∏—è\\n–∏...</td>\n",
       "      <td>@mememe partiq macedonian world history</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>bg_memes_74.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>–ù–∏–∫–æ–≥–∞ –Ω–µ –µ –±–∏–ª–∞!\\n–ß–∞–∫–∞–π, –ó–µ–º—è—Ç–∞ –Ω–µ\\n–µ –º–∞–∫–µ–¥–æ–Ω...</td>\n",
       "      <td>wait, the earth isn't macedonian?</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>bg_memes_75.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>–¢–∞—Ç–µ –¢–∞—Ç–µ || –¥–∞–∞\\n–ê–∑ —Å—ä–º\\n–ú–∞–∫–µ–¥–æ–Ω—Ñ—Ü\\nthe\\n–©–µ –≥...</td>\n",
       "      <td>dad, i'm the macedonfz, you're gonna burn at y...</td>\n",
       "      <td>9</td>\n",
       "      <td>76</td>\n",
       "      <td>bg_memes_76.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>–ú–ê–ö–ï–î–û–ù–ò–Ø –∏\\n–ë–™–õ–ì–ê–†–ò–Ø\\n–ú–ê–ö–ï–î–û–ù–ò–Ø –∏\\n- –ë–™–õ–ì–ê–†–ò–Ø...</td>\n",
       "      <td>macedonia and bulgaria macedonia and - bulgari...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>bg_memes_8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>–ê–ú–ï–†–ò–ö–ê–ù–°–ö–ê–¢–ê –ú–ï–ß–¢–ê\\nglasspar\\nDoughboy\\nCHATI...</td>\n",
       "      <td>american bears glasspar doughboy chaties chiem...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>bg_memes_9.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    –î–ù–ï–í–ù–ò–ö\\n\\n–ö–ê–ü–ò–¢–ê–õ\\n\\nAMERICA FOR BULGARIA FOU...   \n",
       "1    –ö–æ–≥–∞—Ç–æ —Å—ä–º —Å–µ –≤–∞–∫—Å–∏–Ω–∏—Ä–∞–ª –∏ —Å–µ –≤—ä—Ä–Ω–∞ –Ω–∞ —Å–µ–ª–æ –¥–∞...   \n",
       "2    –ì–∞–Ω—è, –≤–∞–∫—Å–∏–Ω–∏—Ä–∞–π —Å–µ, –≥–ª—É–ø–∞–∫–æ!!\\n\\n–ì–∞–Ω—è, –ø–æ–ª—É–¥—è...   \n",
       "3    –ù–µ –º–æ–∂–µ—à –ª–∏ –¥–∞ —Ä–∞–±–æ—Ç–∏—à –Ω–µ—â–æ –Ω–æ—Ä–º–∞–ª–Ω–æ –∫–∞—Ç–æ –¥—Ä—É–≥...   \n",
       "4    –ù–µ –Ω–∞–∑–µ–ª–µ–Ω–∏—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç\\n\\n–í–∞–∫—Å–∏–Ω–∏—Ä–∞–Ω —Å—ä–º\\n–ü—Ä–∏...   \n",
       "..                                                 ...   \n",
       "431  @MEME PARTIQ\\n–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∞\\n–°–≤–µ—Ç–æ–≤–Ω–∞\\n–∏—Å—Ç–æ—Ä–∏—è\\n–∏...   \n",
       "432  –ù–∏–∫–æ–≥–∞ –Ω–µ –µ –±–∏–ª–∞!\\n–ß–∞–∫–∞–π, –ó–µ–º—è—Ç–∞ –Ω–µ\\n–µ –º–∞–∫–µ–¥–æ–Ω...   \n",
       "433  –¢–∞—Ç–µ –¢–∞—Ç–µ || –¥–∞–∞\\n–ê–∑ —Å—ä–º\\n–ú–∞–∫–µ–¥–æ–Ω—Ñ—Ü\\nthe\\n–©–µ –≥...   \n",
       "434  –ú–ê–ö–ï–î–û–ù–ò–Ø –∏\\n–ë–™–õ–ì–ê–†–ò–Ø\\n–ú–ê–ö–ï–î–û–ù–ò–Ø –∏\\n- –ë–™–õ–ì–ê–†–ò–Ø...   \n",
       "435  –ê–ú–ï–†–ò–ö–ê–ù–°–ö–ê–¢–ê –ú–ï–ß–¢–ê\\nglasspar\\nDoughboy\\nCHATI...   \n",
       "\n",
       "                                              aug_text  label  \\\n",
       "0    derivative tier 1 america for bulgaria foundat...      9   \n",
       "1    when i was vaccinated and returned to the vill...      9   \n",
       "2       ganya, you're inoculating yourself, you idiot!      9   \n",
       "3    can't you work something normal like other peo...      9   \n",
       "4    no green certificate i've been vaccinated i've...      9   \n",
       "..                                                 ...    ...   \n",
       "431            @mememe partiq macedonian world history      9   \n",
       "432                  wait, the earth isn't macedonian?      9   \n",
       "433  dad, i'm the macedonfz, you're gonna burn at y...      9   \n",
       "434  macedonia and bulgaria macedonia and - bulgari...      9   \n",
       "435  american bears glasspar doughboy chaties chiem...      9   \n",
       "\n",
       "                  id             image_ID  \n",
       "0     bg_mk_memes_83   bg_mk_memes_83.png  \n",
       "1    bg_mk_memes_104  bg_mk_memes_104.png  \n",
       "2    bg_mk_memes_106  bg_mk_memes_106.png  \n",
       "3    bg_mk_memes_148  bg_mk_memes_148.png  \n",
       "4    bg_mk_memes_237  bg_mk_memes_237.png  \n",
       "..               ...                  ...  \n",
       "431               74      bg_memes_74.png  \n",
       "432               75      bg_memes_75.png  \n",
       "433               76      bg_memes_76.png  \n",
       "434                8       bg_memes_8.png  \n",
       "435                9       bg_memes_9.png  \n",
       "\n",
       "[436 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_bulg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e0b4d6-c395-4940-a188-4f465fe54019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0849bb03-0cf7-47c0-a7ba-a118da896277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]C:\\Users\\tejas\\anaconda3\\envs\\HypEmo\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:23<00:00,  1.40s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259/259 [00:00<00:00, 1763.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('test_data/mk_subtask2a_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df_test_mace =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_mace[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-mk-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())\n",
    "\n",
    "df_test_mace[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d34e93-66e2-4576-808d-e46b8f952112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = pd.concat([df_test_eng_full,df_test_arab,df_test_bulg,df_test_mace],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cfdec47-1cf5-4c35-986e-fd3f38ebf71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aug_text</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>image_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicola Sturgeon\\n\\nWE'RE SCOTTISH GETUSOUTOFHE...</td>\n",
       "      <td>nicola sturgeon\\n\\nwe're scottish getusoutofhe...</td>\n",
       "      <td>9</td>\n",
       "      <td>68254</td>\n",
       "      <td>prop_meme_8883.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw a movie once where only the police and m...</td>\n",
       "      <td>i saw a movie once where only the police and m...</td>\n",
       "      <td>9</td>\n",
       "      <td>69640</td>\n",
       "      <td>prop_meme_10818.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heaven has a Wall and strict immigration polic...</td>\n",
       "      <td>heaven has a wall and strict immigration polic...</td>\n",
       "      <td>9</td>\n",
       "      <td>71251</td>\n",
       "      <td>prop_meme_7048.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don't expect a broken government to fix itself.</td>\n",
       "      <td>don't expect a broken government to fix itself.</td>\n",
       "      <td>9</td>\n",
       "      <td>79369</td>\n",
       "      <td>prop_meme_15611.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOW MOST AMERICANS SEE THE DEBATE\\n\\nHOW FREE ...</td>\n",
       "      <td>how most americans see the debate\\n\\nhow free ...</td>\n",
       "      <td>9</td>\n",
       "      <td>69351</td>\n",
       "      <td>prop_meme_10392.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>–ù–µ–≤–∞–∫—Ü–∏–Ω–∏—Ä–∞–Ω–∏—Ç–µ\\n—Å–µ –ø–æ—Ç–µ–Ω—Ü–∏—ò–∞–ª–Ω–∏\\n—Å–∞–º–æ—É–±–∏—ò—Ü–∏ –∏...</td>\n",
       "      <td>invalids are potential suicides and will not e...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_319</td>\n",
       "      <td>mk_memes_319.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>–ò –í–û –í–¢–û–†–ê–¢–ê –°–í–ï–¢–°–ö–ê –í–û–à–ù–ê –ò–ú–ê–®–ï –ù–ê–®–ò –®–¢–û –û–î–ï–ê...</td>\n",
       "      <td>and in the second world war, ours was what wen...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_321</td>\n",
       "      <td>mk_memes_321.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>–ú–µ—Ç–æ–¥–∏—ò–∞ –ê–Ω–¥–æ–Ω–æ–≤\\n–ß–µ–Ω—Ç–æ –±–∏–ª –∫–∞–ª–∫—É–ª–∞–Ω—Ç.\\n–î—É—Ä–∏ –æ...</td>\n",
       "      <td>methodiusa andonov chento was a calculator, on...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_323</td>\n",
       "      <td>mk_memes_323.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>–ù–ï–ú–ê –í–ï–å–ï –î–ê–¢–£–ú\\n\\n</td>\n",
       "      <td>no more date</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_324</td>\n",
       "      <td>mk_memes_324.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>–í–æ–ª–∫–æ—Ç –æ–¥ —à—É–º–∞—Ç–∞\\n\\n–ê–∫–æ –ë—É–≥–∞—Ä–∏—ò–∞ –∏–∑–ª–µ–∑–µ—à–µ –æ–¥ –ï...</td>\n",
       "      <td>a wolf from the forest if bulgaria had come ou...</td>\n",
       "      <td>9</td>\n",
       "      <td>mk_memes_325</td>\n",
       "      <td>mk_memes_325.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2315 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Nicola Sturgeon\\n\\nWE'RE SCOTTISH GETUSOUTOFHE...   \n",
       "1     I saw a movie once where only the police and m...   \n",
       "2     Heaven has a Wall and strict immigration polic...   \n",
       "3       Don't expect a broken government to fix itself.   \n",
       "4     HOW MOST AMERICANS SEE THE DEBATE\\n\\nHOW FREE ...   \n",
       "...                                                 ...   \n",
       "2310  –ù–µ–≤–∞–∫—Ü–∏–Ω–∏—Ä–∞–Ω–∏—Ç–µ\\n—Å–µ –ø–æ—Ç–µ–Ω—Ü–∏—ò–∞–ª–Ω–∏\\n—Å–∞–º–æ—É–±–∏—ò—Ü–∏ –∏...   \n",
       "2311  –ò –í–û –í–¢–û–†–ê–¢–ê –°–í–ï–¢–°–ö–ê –í–û–à–ù–ê –ò–ú–ê–®–ï –ù–ê–®–ò –®–¢–û –û–î–ï–ê...   \n",
       "2312  –ú–µ—Ç–æ–¥–∏—ò–∞ –ê–Ω–¥–æ–Ω–æ–≤\\n–ß–µ–Ω—Ç–æ –±–∏–ª –∫–∞–ª–∫—É–ª–∞–Ω—Ç.\\n–î—É—Ä–∏ –æ...   \n",
       "2313                                –ù–ï–ú–ê –í–ï–å–ï –î–ê–¢–£–ú\\n\\n   \n",
       "2314  –í–æ–ª–∫–æ—Ç –æ–¥ —à—É–º–∞—Ç–∞\\n\\n–ê–∫–æ –ë—É–≥–∞—Ä–∏—ò–∞ –∏–∑–ª–µ–∑–µ—à–µ –æ–¥ –ï...   \n",
       "\n",
       "                                               aug_text  label            id  \\\n",
       "0     nicola sturgeon\\n\\nwe're scottish getusoutofhe...      9         68254   \n",
       "1     i saw a movie once where only the police and m...      9         69640   \n",
       "2     heaven has a wall and strict immigration polic...      9         71251   \n",
       "3       don't expect a broken government to fix itself.      9         79369   \n",
       "4     how most americans see the debate\\n\\nhow free ...      9         69351   \n",
       "...                                                 ...    ...           ...   \n",
       "2310  invalids are potential suicides and will not e...      9  mk_memes_319   \n",
       "2311  and in the second world war, ours was what wen...      9  mk_memes_321   \n",
       "2312  methodiusa andonov chento was a calculator, on...      9  mk_memes_323   \n",
       "2313                                       no more date      9  mk_memes_324   \n",
       "2314  a wolf from the forest if bulgaria had come ou...      9  mk_memes_325   \n",
       "\n",
       "                 image_ID  \n",
       "0      prop_meme_8883.png  \n",
       "1     prop_meme_10818.png  \n",
       "2      prop_meme_7048.png  \n",
       "3     prop_meme_15611.png  \n",
       "4     prop_meme_10392.png  \n",
       "...                   ...  \n",
       "2310     mk_memes_319.png  \n",
       "2311     mk_memes_321.png  \n",
       "2312     mk_memes_323.png  \n",
       "2313     mk_memes_324.png  \n",
       "2314     mk_memes_325.png  \n",
       "\n",
       "[2315 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_full\n",
    "df_test_full.to_csv('Merged_test_for_HypEmo_2b.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
