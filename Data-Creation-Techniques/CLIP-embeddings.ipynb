{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeb32d3-aef1-4380-9825-b5e8a36a28b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/f4/f0/21efef51304172736b823689aaf82f33dbc64f54e9b046b75f5212d5cee7/ftfy-6.2.0-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (4.66.1)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy)\n",
      "  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/54.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.4/54.4 kB 960.7 kB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, ftfy\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.8\n",
      "    Uninstalling wcwidth-0.2.8:\n",
      "      Successfully uninstalled wcwidth-0.2.8\n",
      "Successfully installed ftfy-6.2.0 wcwidth-0.2.13\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\tejas\\appdata\\local\\temp\\pip-req-build-anxbaqg5\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from clip==1.0) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from clip==1.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from clip==1.0) (0.13.1+cu116)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from torch->clip==1.0) (4.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from torchvision->clip==1.0) (1.26.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from torchvision->clip==1.0) (10.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from requests->torchvision->clip==1.0) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejas\\anaconda3\\envs\\hypemo\\lib\\site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369578 sha256=25d7a64812a7e2ff9ab27808ec9ec7404ebd2714c94b8e16d602bc320590aff5\n",
      "  Stored in directory: C:\\Users\\tejas\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-gqpcgsye\\wheels\\c8\\e4\\e1\\11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\tejas\\AppData\\Local\\Temp\\pip-req-build-anxbaqg5'\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b92925-80c7-46a9-b612-2f093a741705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "dict_x = {}\n",
    "train_images = []\n",
    "train_images_names = []\n",
    "directory = 'test_images/north_macedonian'\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in tqdm(sorted(os.listdir(directory))):\n",
    "    image_name = filename\n",
    "    print(image_name)\n",
    "    image_path = os.path.join(directory, filename)\n",
    "    train_images_names.append(image_name)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    train_images.append(preprocess(image))\n",
    "    \n",
    "import torch\n",
    "image_input = torch.tensor(np.stack(train_images)).cuda()\n",
    "\n",
    "image_features_list = []\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    image_features_list.append(image_features)\n",
    "\n",
    "stacked_tensor = torch.cat(image_features_list,axis=0)\n",
    "from torch.utils.data import TensorDataset\n",
    "dataset = TensorDataset(stacked_tensor)\n",
    "torch.save(dataset, 'test_dataset_mk.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb97435-ae3c-4ca8-8c23-13a0c9781ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = torch.load('test_dataset_mk.pt')\n",
    "directory = 'test_images/north_macedonian'\n",
    "loadeall_data = loaded_dataset[:]\n",
    "loadeall_data = loadeall_data[0]\n",
    "\n",
    "\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    image_name = filename\n",
    "    image_path = os.path.join(directory, filename)\n",
    "    train_images_names.append(image_name)\n",
    "    \n",
    "for i,name in enumerate(train_images_names):\n",
    "    dict_x[name] = loadeall_data[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "932681b1-107b-4c5c-bd3a-f6ece661ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('Merged_test_for_HypEmo_2b.csv').drop('Unnamed: 0',axis=1)\n",
    "training_image_tensor = []\n",
    "for id in training_image_ids:\n",
    "    training_image_tensor.append(dict_x[id])\n",
    "\n",
    "c_tensor = torch.stack(training_image_tensor, dim=0)\n",
    "x = TensorDataset(c_tensor)\n",
    "torch.save(x, 'testing_dataset_catenated.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
