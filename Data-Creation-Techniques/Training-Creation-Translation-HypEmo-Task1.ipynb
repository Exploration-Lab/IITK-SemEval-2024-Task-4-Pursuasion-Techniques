{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fc4f36-1be8-4e70-aba9-fbb0ea186624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "## Training set preperation\n",
    "dict_mapping = {'Presenting Irrelevant Data (Red Herring)': 0,\n",
    " 'Bandwagon': 1,\n",
    " 'Smears': 2,\n",
    " 'Glittering generalities (Virtue)': 3,\n",
    " 'Causal Oversimplification': 4,\n",
    " 'Whataboutism': 5,\n",
    " 'Loaded Language': 6,\n",
    " 'Exaggeration/Minimisation': 7,\n",
    " 'Repetition': 8,\n",
    " 'Thought-terminating clich√©': 9,\n",
    " 'Name calling/Labeling': 10,\n",
    " 'Appeal to authority': 11,\n",
    " 'Black-and-white Fallacy/Dictatorship': 12,\n",
    " 'Obfuscation, Intentional vagueness, Confusion': 13,\n",
    " 'Reductio ad hitlerum': 14,\n",
    " 'Appeal to fear/prejudice': 15,\n",
    " \"Misrepresentation of Someone's Position (Straw Man)\": 16,\n",
    " 'Flag-waving': 17,\n",
    " 'Slogans': 18,\n",
    " 'Doubt': 19,\n",
    " 'Bandwagon_1': 20,\n",
    " 'Whataboutism_1': 21,\n",
    " 'Appeal to fear/prejudice_1': 22,\n",
    " 'Flag-waving_1': 23,\n",
    " 'Appeal to authority_1': 24,\n",
    "}\n",
    "\n",
    "\n",
    "def dataset_f(data,dict_mapping):\n",
    "    set_x = set()\n",
    "    for element in data:\n",
    "        for label in element['labels']:\n",
    "            set_x.add(label)\n",
    "    \n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"labels\"] = element[\"labels\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        if len(element['labels']):\n",
    "            data_id.append(dict_x)\n",
    "    \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    for element in data_id:\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        pattern = r'\\\\n'\n",
    "        cleaned_text = re.sub(pattern, ' ',cleaned_text)\n",
    "        pattern = r'[^a-zA-Z0-9\\s]+'\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text).lower()\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        if(cleaned_text != ''):\n",
    "            unprocessed.append(element['text'])\n",
    "            processed.append(cleaned_text)\n",
    "            labels.append(element['labels'])\n",
    "            ID.append(element['id'])\n",
    "        \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        l_labels = row['label']\n",
    "        empty_list = []\n",
    "        for label in l_labels:\n",
    "            empty_list.append(dict_mapping[label])\n",
    "        row['label'] = empty_list\n",
    "\n",
    "    return df\n",
    "    \n",
    "with open(r'data_given_to_work_with/subtask1/train.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_train = dataset_f(data,dict_mapping)\n",
    "\n",
    "with open(r'data_given_to_work_with/subtask1/validation.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df_val = dataset_f(data,dict_mapping)\n",
    "\n",
    "\n",
    "df_train_full = pd.concat([df_train,df_val],axis=0)\n",
    "df_exploded_train = df_train_full.explode('label')\n",
    "\n",
    "condition = df_exploded_train['label'].isin([1, 5, 15, 17, 11])\n",
    "df_duplicated = df_exploded_train[condition].copy()\n",
    "mapping_dict = {1: 20, 5: 21, 15: 22, 17: 23, 11: 24}\n",
    "df_duplicated['label'] = df_duplicated['label'].map(mapping_dict)\n",
    "df_duplicated = df_duplicated.drop_duplicates()\n",
    "result_df = pd.concat([df_exploded_train, df_duplicated], ignore_index=True)\n",
    "result_df.to_csv('Merged_train_for_HypEmo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb126af-a965-42b8-a5ef-f49d79ade842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "def dataset_f(data,dict_mapping):\n",
    "    data_id = []\n",
    "    for element in data:\n",
    "        dict_x = {}\n",
    "        dict_x[\"text\"] = element[\"text\"]\n",
    "        dict_x[\"id\"] = element[\"id\"]\n",
    "        data_id.append(dict_x)\n",
    "        \n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    labels = []\n",
    "    ID = []\n",
    "    \n",
    "    print(len(data_id))\n",
    "    for element in data_id:\n",
    "        unprocessed.append(element['text'])\n",
    "        cleaned_text = re.sub(r'&', 'and',element['text'])\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        processed.append(cleaned_text)\n",
    "        x = 9\n",
    "        labels.append(x)\n",
    "        ID.append(element['id'])\n",
    "    \n",
    "    data = {'text': unprocessed, 'aug_text': processed, 'label': labels,'id':ID}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "with open(r'data_given_to_work_with/subtask1/en_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_eng_full = dataset_f(data,dict_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2285bf0-3860-41b5-b252-731d8f68051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('Files_for_task1a_input/ar_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_arab =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_arab[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True))    \n",
    "df_test_arab[\"aug_text\"] = t_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc16571-c447-4cd0-ad7e-bfe00f59104b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('Files_for_task1a_input/bg_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "df_test_bulg =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_bulg[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-bg-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())    \n",
    "df_test_bulg[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849bb03-0cf7-47c0-a7ba-a118da896277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open('Files_for_task1a_input/mk_subtask1_test_unlabeled.json', 'rb') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df_test_mace =  dataset_f(data,dict_mapping)\n",
    "src_text = list(df_test_mace[\"text\"])\n",
    "model_name = \"Helsinki-NLP/opus-mt-mk-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t_text = []\n",
    "batch_size = 16\n",
    "num_samples = len(src_text)\n",
    "translations = []\n",
    "\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    batch_src_text = src_text[i:i + batch_size]\n",
    "    batch_src_text = tokenizer(batch_src_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    batch_translated = model.generate(**batch_src_text).to(\"cpu\")\n",
    "    translations.extend(batch_translated)\n",
    "\n",
    "for t in tqdm(translations):\n",
    "    t_text.append(tokenizer.decode(t, skip_special_tokens=True).lower())\n",
    "\n",
    "df_test_mace[\"aug_text\"] = t_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06d34e93-66e2-4576-808d-e46b8f952112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = pd.concat([df_dev_eng_full,df_test_eng_full,df_test_arab,df_test_bulg,df_test_mace],ignore_index=True)\n",
    "import pandas as pd\n",
    "df_test_full = pd.read_csv('Merged_test_for_HypEmo.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
